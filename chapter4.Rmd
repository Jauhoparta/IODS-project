---
title: "Chapter 4"
author: "Tuomas Mäkelä"
date: "24 11 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
```

# Week 4 is for classes and fications.

First of all, let's see the data we're about to classify and look at it.

## Datatataa!

```{r }

str(Boston)
summary(Boston)
```
The data looks to be numeric, with some integer variables (chas and rad). The data is on residential areas in the Boston area, with the main application of the data being to explore some aspects of home ownership or living in Boston. The variable chas is 1 if the home is close to the river, 0 otherwise and the variable rad is related to the accessibility to highways, with values ranging from 1 to 24.

### Plots

The plots have been drawn with chas (the dummy variable) as color, since showing it on the plot itself is just plain wrong. The data has so many variables that four tables were necessary to get all the correlations into the picture.


```{r results='hide'}
ggpairs(Boston, columns=c(1,2,3,5,6,7), mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Boston[5:10], mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Boston[7:14], mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Boston[-(4:10)], mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
```

Looking at the distributions of the variables it seems quite clear that the data is not normally distributed. The peakiness of several of the variables, not to mention the obvious skewedness.

The correlations are high for some of the variables, above 0,7 for
* nox and the set indus, age and dis 
* dis and age
* tax and rad
* istat and medv

This means to say that these variables are likely to have an effect when we do the LDA (if we get to it. . .)

**But** first we have to standardize the data. This means we scale it so that all of it has mean 0 and standard deviation 1:

```{r}
Data<-as.data.frame(scale(Boston))
summary(Data)


```
The obvious thing to change is the mean, but let's look at graphical representations of a couple of variables, namely nox, age and indus:
```{r echo=FALSE, results='hide'}

par(mar=c(1.1,1.1,1.1,1.1), mfrow=c(1,2))
ggpairs(Boston, columns=c(3,5,7), mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Data, columns=c(3,5,7), mapping = aes(col=as.factor(Data$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
```
   
From this representation the difference is clear: the mean becomes 0, but the shape of the distribution is unchanged. Now our data is initialized and we can LDA it. As a final exercise we'll make a categorical crimerate variable and divide the data into test and train sets (no toy trains, though). 
```{r echo=TRUE}
bins <- quantile(Data$crim)
crime <- cut(Data$crim, breaks = bins,label=c("low","med_low","med_high","high"), include.lowest = TRUE)
Data <- dplyr::select(Data, -crim)
Data <- data.frame(Data, crime)
n <- nrow(Data)

ind <- sample(n,  size = n * 0.8)
train <- Data[ind,]
test <- Data[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

```
 
## Linear Discriminant Analysis 

Lets do **LDA**!
```{r echo=T}
# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 2)

plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 5)

```

Nice plots. The second one is there so we can actually read the other variables than rad, nox and zn. . .Let me remark that it's quite dubious to do LDA on a set with a binary variable as it's sure not to be Gaussian, but let's not get into that for now. Now lets's test the model for predictions.

From the plot we see that the variale rad seems to be a high predictor for crime rate. Also, as noted the high correlations of nox and age seem to pop up a bit, though they don't seem to be very highly associated with crime.

## Predictions

Let's now predict thigs based on our model.

```{r echo=F}
library(knitr)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The table shows quite good predictions, although the med to low crime rate seems to be hard to predict. If we look at the LDA, we see that the components of med to low are the red ones and it seems that they are sitting right in the middle of the two other categories (low and med high) so that the analysis is a little veiled.

## Klustering with Kmeans

Now for some interesting stuff. Let's see if we get clusters formed.

```{r}
boston<-as.data.frame(scale(Boston))

#Let's see the distances of the variables.
dist_eu <- dist(boston)

# look at the summary of the distances
summary(dist_eu)
km <-kmeans(boston, centers = 3)

# plot the Boston dataset with clusters
par(mfrow=c(2,2))
pairs(boston[c(1,2,3,5,6)], col = km$cluster)
pairs(boston[6:10], col = km$cluster)
pairs(boston[10:14], col = km$cluster)
pairs(boston[c(12,13,14,1,2,3)], col = km$cluster)
# MASS, ggplot2 and Boston dataset are available

```

We don't see much with this, but seems that some of the variables have been clustered together by the algorithm.

