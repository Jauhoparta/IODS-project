---
title: "Chapter 4"
author: "Tuomas Mäkelä"
date: "24 11 2019"
output: html_document
---

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
```

# Week 4 is for classes and fications.

First of all, let's see the data we're about to classify and look at it.

## datatataa!

```{r}

str(Boston)
summary(Boston)
```
The data looks to be numeric, with some integer variables (chas and rad). The data is on residential areas in the Boston area, with the main application of the data being to explore some aspects of home ownership or living in Boston. The variable chas is 1 if the home is close to the river, 0 otherwise and the variable rad is related to the accessibility to highways, with values ranging from 1 to 24.

### Plots

The plots have been drawn with chas (the dummy variable) as color, since showing it on the plot itself is just plain wrong. The data has so many variables that four tables were necessary to get all the correlations into the picture.


```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
ggpairs(Boston, columns=c(1,2,3,5,6,7,8,9,10), mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Boston[6:14], mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(Boston[c(11,12,13,14,1,2,3,5)], mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
```

Looking at the distributions of the variables it seems quite clear that the data is not normally distributed. The peakiness of several of the variables, not to mention the obvious skewedness.

The correlations are high for some of the variables, above 0,7 for  

nox and the set indus, age and dis   
dis and age  
tax and rad  
istat and medv  

This means to say that these variables are likely to have an effect when we do the LDA if we get to it.

But first we have to standardize the data. This means we scale it so that all of it has mean 0 and standard deviation 1:

```{r echo=FALSE}

bostondata<-as.data.frame(scale(Boston))
summary(bostondata)
```
  
The obvious thing to change is the mean, but let's look at graphical representations of a couple of variables, namely nox, age and indus:  

```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)

ggpairs(Boston, columns=c(3,5,7), mapping = aes(col=as.factor(Boston$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
ggpairs(bostondata, columns=c(3,5,7), mapping = aes(col=as.factor(bostondata$chas), alpha=0.3), lower = list(combo = wrap("facethist", bins = 20)))
```
   
From this representation the difference is clear: the mean becomes 0, but the shape of the distribution is unchanged. Now our data is initialized and we can LDA it. As a final exercise we'll make a categorical crimerate variable and divide the data into test and train sets (no toy trains, though). 
```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
bostondata<-as.data.frame(scale(Boston))
bins <- quantile(bostondata$crim)
crime <- cut(bostondata$crim, breaks = bins,label=c("low","med_low","med_high","high"), include.lowest = TRUE)
bostondata <- dplyr::select(bostondata, -crim)
bostondata <- data.frame(bostondata, crime)
n <- nrow(bostondata)

ind <- sample(n,  size = n * 0.8)
train <- bostondata[ind,]
test <- bostondata[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

```
 
## Linear Discriminant Analysis 

Lets do **LDA**!
```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
bostondata<-as.data.frame(scale(Boston))
bins <- quantile(bostondata$crim)
crime <- cut(bostondata$crim, breaks = bins,label=c("low","med_low","med_high","high"), include.lowest = TRUE)
bostondata <- dplyr::select(bostondata, -crim)
bostondata <- data.frame(bostondata, crime)
n <- nrow(bostondata)

ind <- sample(n,  size = n * 0.8)
train <- bostondata[ind,]
test <- bostondata[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 2)

plot(lda.fit, dimen = 2, col=classes, pch=classes)
lda.arrows(lda.fit, myscale = 5)

```

Nice plots. The second one is there so we can actually read the other variables than rad, nox and zn. . .Let me remark that it's quite dubious to do LDA on a set with a binary variable as it's sure not to be Gaussian, but let's not get into that for now. Now lets's test the model for predictions.

From the plot we see that the variale rad seems to be a high predictor for crime rate. Also, as noted the high correlations of nox and age seem to pop up a bit, though they don't seem to be very highly associated with crime.

## Predictions

Let's now predict thigs based on our model.

```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
library(knitr)
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
```

The table shows quite good predictions, although the med to low crime rate seems to be hard to predict. If we look at the LDA, we see that the components of med to low are the red ones and it seems that they are sitting right in the middle of the two other categories (low and med high) so that the analysis is a little veiled.

## Klustering with Kmeans

Now for some interesting stuff. Let's see if we get clusters formed.

```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
boston<-as.data.frame(scale(Boston))

#Let's see the distances of the variables.
dist_eu <- dist(boston)

# look at the summary of the distances
summary(dist_eu)
km <-kmeans(boston, centers = 3)

# plot the Boston dataset with clusters
par(mfrow=c(2,2))
pairs(boston[c(1,2,3,5,6)], col = km$cluster)
pairs(boston[6:10], col = km$cluster)
pairs(boston[10:14], col = km$cluster)
pairs(boston[c(12,13,14,1,2,3)], col = km$cluster)
# MASS, ggplot2 and Boston dataset are available

```

We don't see much with this, but seems that some of the variables have been clustered together by the algorithm.

Now we do the same with  clusters (5 is just too much, in my expert opinion and looking at the graph below
).
```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')


```

From the plot we see the behaviour of the within clusters sum of squares. As the number of clusters climbs, the sum will sink but we're not looking for a minimum but for a good change, which happens here at 2 clusters (or 4 clusters, depending on the reader.) We'll do the clustering again with 2 clusters:

```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
# k-means clustering
km <-kmeans(boston, centers = 2)

# plot the Boston dataset with clusters
par(mfrow=c(2,2))
pairs(boston[c(1,2,3,5,6)], col = km$cluster)
pairs(boston[6:10], col = km$cluster)
pairs(boston[10:14], col = km$cluster)
pairs(boston[c(12,13,14,1,2,3)], col = km$cluster)
```
The interpretation is not quite clear. We see from the data that now everything seems to be well clustered, but it still might be more informative if we apply 4 clusters. Lets. 

```{r}
library(MASS)
library(ggplot2)
library(GGally)
library(dplyr)
km <-kmeans(boston, centers = 4)

# plot the Boston dataset with clusters
par(mfrow=c(2,2))
pairs(boston[c(1,2,3,5,6)], col = km$cluster)
pairs(boston[6:10], col = km$cluster)
pairs(boston[10:14], col = km$cluster)
pairs(boston[c(12,13,14,1,2,3)], col = km$cluster)
```
  
Now the data seems nicer to the eye. Most of the clusters do not overlap, and it seems that we gain more insight into the data set with the clustering: for example, the variable indus seems to have one almost categorical value that defines the red cluster, and similarly fo ptratio and tax. The further analysis would require we do a detailed look into the relations of the variables using only the found clusters, but clearly we do not have time for that.
